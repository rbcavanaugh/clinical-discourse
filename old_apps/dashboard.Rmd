---
title: "Automated Core Lexicon Scoring (In Development)"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    social: menu
    source_code: embed
    theme: cosmo
runtime: shiny
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tibble)
library(scales)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
library(plotly)
library(textstem)
library(here)
library(DT)
library(truncnorm)
library(shinyjs)
library(here)

source(here('R', 'core_lex.R'))

```

```{r dat1}
# Reactive that returns the whole dataset if there is no brush
selectedData <- reactive({
  
        task = case_when(
          input$stim == 'broken_window' ~ 1,
          input$stim == 'refused_umbrella' ~ 2,
          input$stim == 'cat_rescue' ~ 3,
          input$stim == 'cinderella' ~ 4,
          input$stim == 'sandwich' ~ 5,
          input$stim == 'gdc' ~ 6,
          input$stim == 'picnic' ~ 7
        )
        
        # ACCURACY ####
        
        norm_mean = case_when(
          input$stim == 'broken_window' ~ list(c(19, 2.9, 12, 23)),
          input$stim == 'refused_umbrella' ~ list(c(26.5, 3.1, 17, 33)),
          input$stim == 'cat_rescue' ~ list(c(26.3, 3.3, 16, 33)),
          input$stim == 'cinderella' ~ list(c(69.8, 15.5, 6, 90)),
          input$stim == 'sandwich' ~ list(c(19, 2.7, 14, 23))
        )
      
        aphasia_mean = case_when(
          input$stim == 'broken_window' ~ list(c(12, 4.9, 0, 22)),
          input$stim == 'refused_umbrella' ~ list(c(17.3, 7.7, 0, 34)),
          input$stim == 'cat_rescue' ~ list(c(16.8, 7.0, 0, 31)),
          input$stim == 'cinderella' ~ list(c(37.5, 19.4, 1, 82)),
          input$stim == 'sandwich' ~ list(c(11.3, 5.4, 0, 23))
        )

        # EFFICIENCY ####
        
        norm_mean_eff = case_when(
          input$stim == 'broken_window' ~ list(c(34.8, 13.5, 6.7, 72.9)),
          input$stim == 'refused_umbrella' ~ list(c(41.3, 14.5, 16.6, 98.2)),
          input$stim == 'cat_rescue' ~ list(c(39.8, 14.3, 13.3, 100)),
          input$stim == 'cinderella' ~ list(c(24.7, 10.0,3.4, 72.0)),
          input$stim == 'sandwich' ~ list(c(40.5, 16.6, 8.6, 114.0))
        )
        
        aphasia_mean_eff = case_when(
          input$stim == 'broken_window' ~ list(c(18.5, 13.3, 0, 84.0)),
          input$stim == 'refused_umbrella' ~ list(c(19.7, 13.4, 0, 67.5)),
          input$stim == 'cat_rescue' ~ list(c(16.9, 12.3, 0, 93.8)),
          input$stim == 'cinderella' ~ list(c(14.1, 9.1, 0.4, 62.5)),
          input$stim == 'sandwich' ~ list(c(24.1,20.1, 0, 156.0))
        )
        
        # Max 
        max_val = case_when(
          input$stim == 'broken_window' ~ 24,
          input$stim == 'refused_umbrella' ~ 35,
          input$stim == 'cat_rescue' ~ 34,
          input$stim == 'cinderella' ~ 94,
          input$stim == 'sandwich' ~ 25
        )
        
        df <- core_lex(input$transcr, task, input$age)
        matches <- df$match
        score_num = sum(matches$produced) + input$adj
        score_eff = score_num/(input$time/60)
        
        dist1 = truncnorm::rtruncnorm(10000,
                                      mean = norm_mean[[1]][[1]],
                                      sd = norm_mean[[1]][[2]],
                                      a = norm_mean[[1]][[3]],
                                      b = norm_mean[[1]][[4]])
        percentile1 = label_percent()(ecdf(dist1)(22))
        
        dist2 = truncnorm::rtruncnorm(10000,
                                      mean = aphasia_mean[[1]][[1]],
                                      sd = aphasia_mean[[1]][[2]],
                                       a = aphasia_mean[[1]][[3]],
                                      b = aphasia_mean[[1]][[4]])
        percentile2 = label_percent()(ecdf(dist2)(score_num))
        
        dist3 = truncnorm::rtruncnorm(10000,
                                      mean = norm_mean_eff[[1]][[1]],
                                      sd = norm_mean_eff[[1]][[2]],
                                      a = norm_mean_eff[[1]][[3]],
                                      b = norm_mean_eff[[1]][[4]])
        percentile3 = label_percent()(ecdf(dist3)(score_eff))
        
        dist4 = truncnorm::rtruncnorm(10000,
                                      mean = aphasia_mean_eff[[1]][[1]],
                                      sd = aphasia_mean_eff[[1]][[2]],
                                      a = aphasia_mean_eff[[1]][[3]],
                                      b = aphasia_mean_eff[[1]][[4]])
        percentile4 = label_percent()(ecdf(dist4)(score_eff))
        
        score = tibble(
          Metric = c('Production', 'Efficiency'),
          Score = c(paste0(round(score_num,0),' core words'),
                    paste0(round(score_eff, 1), ' core words/min')),
          ControlPercentile =  c(percentile1, percentile3),
          AphasiaPercentile = c(percentile2, percentile4)
          )
        
        score <- score %>%
          mutate(
            ControlPercentile = ifelse(Metric != 'Production', ControlPercentile,
                                       ifelse(score_num > max_val,
                                       'exceeded max score',
                                       ControlPercentile)
                                       ),
            AphasiaPercentile = ifelse(Metric != 'Production', AphasiaPercentile,
                                       ifelse(score_num > max_val,
                                              'exceeded max score',
                                              AphasiaPercentile)
            )
          )
        
        colnames(score) <- c('Metric', 'Score', 'Control Percentile', 'Aphasia Percentile')
        
        dists <- tibble(
          dist1 = dist1,
          dist2 = dist2, 
          dist3 = dist3,
          dist4 = dist4
        ) %>%
          pivot_longer(cols = 1:4, names_to = 'dist', values_to = 'val')
        
        core_lex_data <- list()
        core_lex_data[["score"]] = score
        core_lex_data[["dist"]] = dists
        core_lex_data[["scores"]] = c(score_num, score_eff)
        
        return(core_lex_data) # make this a list with the data for the histograms too. 
  
})


```

``` {r dat2}
selectedData2 <- reactive({
      task = case_when(
        input$stim == 'broken_window' ~ 1,
        input$stim == 'refused_umbrella' ~ 2,
        input$stim == 'cat_rescue' ~ 3,
        input$stim == 'cinderella' ~ 4,
        input$stim == 'sandwich' ~ 5,
        input$stim == 'gdc' ~ 6,
        input$stim == 'picnic' ~ 7
      )
      df <- core_lex(input$transcr, task, input$age)
      options = list(show = 10)
      table = df$match %>%
        mutate(produced = ifelse(produced == 1, 'yes', 'no')) %>%
        rename('Target Lexeme' = target_lemma,
               'Lexeme Produced?' = produced,
               'Token' = token)
      return(table)
})
```

Core Lexicon Analysis
=====================================

Inputs{.sidebar data-width=300}
-----------------------------------------------------------------------

```{r}

radioButtons("stim", "Elicitation Task:",
             c("Broken Window" = 'broken_window',
               "Cat Rescue" = 'cat_rescue',
               "Refused Umbrella" = 'refused_umbrella',
               "Cinderella" = 'cinderella',
               "Sandwich" = 'sandwich'),
              # "Good Dog Carl" = 'gdc',
               #"Picnic" = 'picnic'),
             inline = F)
            
textAreaInput("transcr",
              "Transcript:",
              value = "Young boy is practicing playing soccer. Kicking the ball up and keeping it in the air. He miskicks. and and it fall goes and breaks the window of his house. of the living room actually. and bounces into the living room knocking a lamp over where his father is sitting. the father picks up the soccer ball. Looks out the window. And calls for the little boy to come and explain.",
              height = '300px')

sliderInput("time", "Time on task (seconds)", value= 120,
            min = 1, max = 1200, step = 10)

sliderInput("adj", "Score Adjustment:", value= 0,
             min = -10, max = 10, step = 1)

a(icon("copyright"), "Rob Cavanaugh 2021", icon("external-link"), href="https://robcavanaugh.com", target = "_blank")
 
```

Scores 
-----------------------------------------------------------------------

### Check Scoring

#### Use the score adjustment slider to add or subtract a point

1. Check that target lexemes match tokens, following core lexicon rules.
1. Check that target lexemes without a matched token were not missed by the algorithm. (Lexeme Produced = no)
1. Add 1 point to the Cinderella passage if the possessive [ 's ] is used
1. Count any variation of mom/mother or dad/father.\
\
  
```{r}

renderDT({
  table = selectedData2()
})

```

Words
-----------------------------------------------------------------------

### Result {data-height=150}

``` {r}

renderTable({
  score = selectedData()[[1]]
  score
  })

```

### Performance relative to people with and without aphasia {data-height=400}

``` {r}


renderPlot({
  p <- selectedData()[[2]] %>%
    mutate(Cohort = ifelse(dist == 'dist1' | dist == 'dist3', 'control', 'aphasia'),
           met = factor(ifelse(dist == 'dist1' | dist == 'dist2', 'Production', 'Efficiency'),
                        levels = c('Production', 'Efficiency'))
           ) %>%
    ggplot(aes(x = val, color = Cohort, fill = Cohort)) +
    geom_density(alpha = .3) +
    geom_vline(data = data.frame(xint=selectedData()[[3]][[1]],met="Production"), 
               aes(xintercept = xint)) +
    geom_vline(data = data.frame(xint=selectedData()[[3]][[2]],met="Efficiency"), 
               aes(xintercept = xint)) +
    facet_wrap(.~met, scales = 'free') +
    theme_grey(base_size = 16) +
    theme(legend.position = 'bottom',
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          legend.margin=margin(c(.5, .5, .5, .5))) +
    labs(x = 'Score')

  p

  })
```

### References {data-height=300}

Dalton, S. G., Hubbard, H. I., & Richardson, J. D. (2019). Moving toward non-transcription based discourse analysis in stable and progressive aphasia. In Seminars in speech and language. Thieme Medical Publishers.\

Dalton, S. G., & Richardson, J. D. (2015). Core-lexicon and main-concept production during picture-sequence description in adults without brain damage and adults with aphasia. American Journal of Speech-Language Pathology, 24(4), S923-S938.\

Kim, H., & Wright, H. H. (2020, January). A tutorial on core lexicon: development, use, and application. In Seminars in speech and language (Vol. 41, No. 01, pp. 020-031). Thieme Medical Publishers.\

Silge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” JOSS, 1(3). doi: 10.21105/joss.00037




Main Concept Analysis
=====================================

Inputs{.sidebar data-width=300}
-----------------------------------------------------------------------
```{r}

radioButtons("stim", "Elicitation Task:",
             c("Broken Window" = 'broken_window',
               "Cat Rescue" = 'cat_rescue',
               "Refused Umbrella" = 'refused_umbrella',
               "Cinderella" = 'cinderella',
               "Sandwich" = 'sandwich'),
              # "Good Dog Carl" = 'gdc',
               #"Picnic" = 'picnic'),
             inline = F)
            
textAreaInput("transcr",
              "Transcript:",
              value = "Young boy is practicing playing soccer. Kicking the ball up and keeping it in the air. He miskicks. and and it fall goes and breaks the window of his house. of the living room actually. and bounces into the living room knocking a lamp over where his father is sitting. the father picks up the soccer ball. Looks out the window. And calls for the little boy to come and explain.",
              height = '300px')

sliderInput("time", "Time on task (seconds)", value= 120,
            min = 1, max = 1200, step = 10)

sliderInput("adj", "Score Adjustment:", value= 0,
             min = -10, max = 10, step = 1)

a(icon("copyright"), "Rob Cavanaugh 2021", icon("external-link"), href="https://robcavanaugh.com", target = "_blank")
 
```

Main Concept Scoring 
-----------------------------------------------------------------------

### Coming Soon!
